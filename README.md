# DLTLakeflow
LAKEFLOW DECLARATIVE PIPELINES - batch loading ETL pipeline for healthcare data using Databricks Delta Live Tables (DLT) and sliding window functions

<img width="2362" height="1472" alt="image" src="https://github.com/user-attachments/assets/373b7b01-b8cf-49e8-ae9f-a77a4a0c8343" />

1. I have S3 bucket available in AWS with following folders <img width="2338" height="156" alt="image" src="https://github.com/user-attachments/assets/91670bcb-4e30-4736-9a95-facddbd4ab9f" />
2. <img width="2370" height="804" alt="image" src="https://github.com/user-attachments/assets/173a9922-c366-4332-afd7-12efabe51789" />
3. <img width="2380" height="798" alt="image" src="https://github.com/user-attachments/assets/5263cfba-c110-401d-ba6e-780601c0cdc1" />
4. <img width="2172" height="770" alt="image" src="https://github.com/user-attachments/assets/183a3c9b-4e8e-418c-a788-e556f0d154a9" />
5. <img width="2542" height="840" alt="image" src="https://github.com/user-attachments/assets/06d3039e-e22e-467f-8bf7-dccf95ea465b" />
6. Here I am using python to create pipeline
7. After creation of ETL, folder structure will be like below. I put appropriate name for folders and files
8. <img width="2632" height="808" alt="image" src="https://github.com/user-attachments/assets/3ac21d27-9f74-4914-a943-1350bd491a36" />






